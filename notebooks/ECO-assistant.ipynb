{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Project Overview \n",
    " \n",
    " #### 🧠HappyMatrix ECO Assistant\n",
    " ***A GenAI-Powered Change Order Analysis Tool***  \n",
    "\n",
    "\n",
    "\n",
    "**Author**: Olga Seymour\n",
    "\n",
    "**Date**: May 2025  \n",
    "\n",
    "**GitHub**: https://github.com/data-ai-studio/happymatrix-eco-assistant\n",
    "\n",
    "\n",
    " \n",
    "I created a fictional fitness technology company called **HappyMatrix** and developed a conceptual fitness tracker product named **MatrixSync X100**. All company information, product specifications, and engineering documentation are original creations designed to demonstrate product development processes in the wearable technology space while avoiding any infringement on existing intellectual property. The MatrixSync X100 represents a forward-thinking approach to fitness tracking that addresses current industry challenges including thermal management, battery technology, algorithm accuracy, and sustainability.\n",
    "\n",
    "*Note*: **HappyMatrix** and **MatrixSync X100** are fictional entities created solely for educational purposes. Any similarity to existing companies or products is coincidental. This project focuses on demonstrating engineering change management through realistic but entirely fictional documentation.\n",
    "\n",
    "### Use Case and Inspiration\n",
    "This project demonstrates how Generative AI can assist engineers and program managers in understanding and organizing **Engineering Change Orders (ECOs)**. It is inspired by my previous experience at **Fitbit (Google)**, where I worked in product data management, supporting documentation control, BOM updates, and engineering change processes.\n",
    "\n",
    "Although all data here is **synthetic**, the assistant reflects real-world workflows from **hardware development, product lifecycle management**, and **engineering program management**. It simulates how GenAI could be applied to engineering operations to **automate extraction, summarization, and communication** of key insights from unstructured ECO documents.\n",
    "\n",
    "### Project Objective\n",
    "This notebook presents a Generative AI-powered assistant built using:\n",
    "\n",
    "- **Google Gemini LLMs** (via LangChain)\n",
    "\n",
    "- **LangChain** for RAG orchestration\n",
    "\n",
    "- **ChromaDB** for semantic vector retrieval\n",
    "\n",
    "The assistant helps users analyze ECO documents by answering questions, extracting structured data, and drafting communication summaries — all based on the content retrieved from vectorized ECO files.\n",
    "\n",
    "### Problem Statement\n",
    "ECOs document important product and engineering changes, but they are often:\n",
    "\n",
    "- Unstructured and inconsistent in format\n",
    "\n",
    "- Time-consuming to analyze manually\n",
    "\n",
    "- Difficult to integrate into downstream tools or dashboards\n",
    "\n",
    "The assistant addresses these issues using **Gemini + LangChain + ChromaDB** to automate document understanding and response generation.\n",
    "\n",
    "### Project Goals\n",
    "Build a Generative AI assistant that can:\n",
    "\n",
    "- Answer questions about ECOs using document understanding\n",
    "\n",
    "- Extract structured summaries in JSON format\n",
    "\n",
    "- Generate professional stakeholder email summaries\n",
    "\n",
    "- Demonstrate how RAG, few-shot prompting, and structured output work together\n",
    "\n",
    "- Enable semantic search over ECO documents\n",
    "\n",
    "- Present results in both natural language and structured formats\n",
    "\n",
    "- Simulate real-world GenAI applications in engineering workflows\n",
    "\n",
    "**Disclaimer**: All ECO documents are synthetic and created solely for educational purposes. They do not reflect any real product or proprietary information from any company, including Fitbit.\n",
    "\n",
    "### Assistant Capabilities\n",
    "The assistant supports:\n",
    "\n",
    "- Natural language Q&A over ECO documents\n",
    "\n",
    "- Structured JSON extraction for downstream use\n",
    "\n",
    "- Automated stakeholder email generation\n",
    "\n",
    "- Evaluation of response quality using Gemini\n",
    "\n",
    "### GenAI Capabilities Demonstrated\n",
    "\n",
    "✅ Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "✅ Few-shot prompting\n",
    "\n",
    "✅ Structured output (JSON mode)\n",
    "\n",
    "✅ Gemini-powered evaluation\n",
    "\n",
    "✅ Real-world document understanding\n",
    "\n",
    "✅ Agent routing, GenAI evaluation, stakeholder email generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Overview\n",
    "\n",
    "| Capability                        | Where It's Used         |\n",
    "|----------------------------------|--------------------------| \n",
    "| **Few-shot prompting**           | Step 7                  |\n",
    "| **Document understanding (RAG)** | Steps 4–7               |\n",
    "| **Structured output/JSON mode**  | Steps 8, 9, 12|\n",
    "| **Agents**                       | Steps 10–11              |\n",
    "| **GenAI evaluation**             | Step 13                 |\n",
    "| **Embeddings + vector search**   | Steps 4–6                |\n",
    "\n",
    "### Tools Used\n",
    "\n",
    "- langchain\n",
    "- langchain-google-genai\n",
    "- chromadb\n",
    "- google-generativeai\n",
    "- python-dotenv\n",
    "\n",
    "\n",
    "### Technical Challenges and Solutions\n",
    "\n",
    "| **Challenge**                             | **Solution**                                                                 |\n",
    "|------------------------------------------|-----------------------------------------------------------------------------| \n",
    "| Hallucination when asking vague questions| Used strict prompts and few-shot examples to anchor responses               |\n",
    "| Mixing answers across ECOs               | Rewrote prompts to be ECO-specific and added structured post-processing     |\n",
    "| API rate limits during testing           | Added `time.sleep()` between requests and optimized retriever settings      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Architecture\n",
    "\n",
    "This assistant follows a modular design with these key components:\n",
    "\n",
    "1. **ECOAssistant Class**: Core implementation with methods for document processing, query handling, and output formatting\n",
    "2. **Vector Database**: ChromaDB store containing embeddings of ECO documents for semantic search\n",
    "3. **Helper Functions**: Utilities for document loading, validation, and error handling\n",
    "4. **Configuration**: Central settings management for model parameters and processing options\n",
    "\n",
    "The system uses Retrieval-Augmented Generation (RAG) to ground all responses in the original ECO documents, ensuring accuracy and preventing hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Walkthrough\n",
    "\n",
    "#### 1. Environment Setup\n",
    "First, we install required dependencies and configure the environment:\n",
    "- Install LangChain extensions and ChromaDB\n",
    "- Import necessary libraries\n",
    "- Set up secure API key access\n",
    "\n",
    "#### 2. Document Processing\n",
    "The foundation of our assistant is document understanding:\n",
    "- Load and tag synthetic ECO documents with their identifiers\n",
    "- Split documents into overlapping chunks for better semantic matching\n",
    "- Create vector embeddings for each chunk using Gemini's embedding model\n",
    "- Store embeddings in ChromaDB for fast retrieval\n",
    "\n",
    "#### 3. Core Assistant Class\n",
    "The `ECOAssistant` class handles:\n",
    "- Document loading and vectorization\n",
    "- Q&A chain building with few-shot prompting\n",
    "- Query processing and response generation\n",
    "- Structured JSON extraction\n",
    "- Format selection based on user intent\n",
    "\n",
    "#### 4. Advanced Features\n",
    "Building on the core functionality:\n",
    "- **Batch Processing**: Process multiple ECOs with a single call\n",
    "- **Agent Routing**: Automatically detect and route to the appropriate response format\n",
    "- **Format Toggling**: Switch between JSON and natural language based on user preference\n",
    "- **Response Evaluation**: Compare assistant outputs to reference answers\n",
    "- **Stakeholder Email Generation**: Create professional communication from technical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Capabilities Demonstrated\n",
    "\n",
    "| Capability | Implementation | Description |\n",
    "|------------|----------------|-------------|\n",
    "| **Retrieval-Augmented Generation (RAG)** | `ECOAssistant.query()` | Grounds responses in actual ECO documents for factual accuracy |\n",
    "| **Few-shot prompting** | `build_qa_chain()` | Uses example Q&A pairs to improve extraction consistency |\n",
    "| **Structured output (JSON)** | `get_structured_output()` | Formats data for downstream systems and automation |\n",
    "| **Agent-based routing** | `route_query()` | Intelligently selects output format based on query intent |\n",
    "| **Evaluation** | `evaluate_answer()` | Uses Gemini to assess response quality against references |\n",
    "| **Business communication** | `generate_stakeholder_email()` | Creates professional stakeholder communications from technical data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact\n",
    "This project demonstrates how Generative AI combined with vector search can support engineering teams in analyzing Engineering Change Orders (ECOs) at scale. \n",
    "\n",
    "**Key accomplishments include**:\n",
    "\n",
    "- Accurate, context-aware Q&A using Gemini + RAG\n",
    "\n",
    "- Structured JSON output for downstream automation and dashboards\n",
    "\n",
    "- Business-ready email generation for stakeholder communication\n",
    "\n",
    "- Flexible output routing logic (JSON vs. natural language) to support both technical and non-technical users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Install Required LangChain Extensions\n",
    "This step installs additional LangChain and third-party libraries needed for Gemini integration, vector storage, and RAG workflows:\n",
    "\n",
    "- **`langchain-community`**: Provides access to community-supported LangChain tools, including Chroma support.\n",
    "- **`langchain-google-genai`**: Enables Gemini model support within LangChain (e.g., for embeddings and chat).\n",
    "- **`chromadb`**: A vector database used for storing and retrieving document embeddings.\n",
    "- **`google-cloud-automl`**: Uninstalled due to known dependency conflicts with other required packages.\n",
    "\n",
    "These packages are necessary to enable document splitting, embedding with Gemini, vector storage with Chroma, and query-answering with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First uninstall google-cloud-automl to avoid protobuf conflicts\n",
    "# This was causing dependency headaches with ChromaDB\n",
    "!pip uninstall -qqy google-cloud-automl\n",
    "\n",
    "# Install the core packages for our RAG implementation\n",
    "# Using -q flag to keep the notebook clean \n",
    "!pip install -q langchain-community \n",
    "!pip install -q langchain-google-genai\n",
    "!pip install -q chromadb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Import Libraries for GenAI and Document Processing\n",
    "\n",
    "This step prepares the notebook with the required tools for working with Gemini models, vector embeddings, and document processing:\n",
    "\n",
    "- **LangChain + Gemini Integration**: Enables access to Gemini models within LangChain workflows.\n",
    "- **Chroma**: A vector database used to store and retrieve document embeddings.\n",
    "- **GoogleGenerativeAIEmbeddings**: Gemini's `embedding-001` model to turn text into vector representations.\n",
    "- **TextLoader and RecursiveCharacterTextSplitter**: For loading ECO documents and splitting them into manageable chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs for file and data handling \n",
    "import os\n",
    "import json\n",
    "import re \n",
    "import glob\n",
    "import shutil \n",
    "import time   \n",
    "import pandas as pd\n",
    "from pprint import pprint  \n",
    "\n",
    "# Gemini and LangChain - the core RAG components\n",
    "# These enable our semantic search over ECO docs  \n",
    "from google import genai\n",
    "from google.genai import types  \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Load API Key Securely \n",
    "To keep the Gemini API key secure and out of the code, I stored it in a separate environment file. This step loads the key into the notebook so it can be used to authenticate Gemini API calls throughout the project.\n",
    "\n",
    "Using environment variables avoids hardcoding sensitive credentials and ensures best practices for secure access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Based on Code Lab pattern — adjusted to load key from a custom .env file\n",
    "\n",
    "# API key handling - always use env vars for credentials\n",
    "# Never hardcode API keys in production code! \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load Gemini API key from environment file\n",
    "# For GitHub users: Create a .env file following the .env.example template\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Get the API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if key is available\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Warning: No API key found. Please create a .env file with your GOOGLE_API_KEY.\")\n",
    "    raise ValueError(\"API key not found. Make sure it's set in the .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Preparing the Vector Environment\n",
    "\n",
    "Before creating a new vector database, ensure any existing ChromaDB stores are removed to prevent conflicts or corrupted indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing vector store at: eco_chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Source: Based on ChromaDB usage patterns\n",
    "# Modified to fit the project setup\n",
    "\n",
    "# Clean slate approach - remove any old vector DB \n",
    "# This prevents stale embeddings or index corruption\n",
    "persist_dir = \"eco_chroma_db\"\n",
    "\n",
    "    \n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "    print(f\"Deleted existing vector store at: {persist_dir}\")\n",
    "else:\n",
    "    print(f\"No vector store found at: {persist_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Load and Tag Documents¶\n",
    "In this step, I load synthetic ECO text files and tag them with their ECO numbers, which helps the retriever later identify document context.\n",
    "\n",
    "This step simulates document ingestion and prepares the assistant for semantic search\n",
    "Demonstrates early preparation for Document Understanding and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Embed ECO Documents into a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I convert synthetic ECO documents into vector embeddings to support semantic retrieval during Q&A:\n",
    "\n",
    "-  Split documents into overlapping chunks using RecursiveCharacterTextSplitter (750 chars, 250 overlap)\n",
    "\n",
    "- Wrapped each chunk as a LangChain Document\n",
    "\n",
    "-  Used Gemini’s embedding-001 model to generate vector representations\n",
    "\n",
    "-  Stored the embeddings in ChromaDB for fast retrieval\n",
    "\n",
    "This enables RAG (Retrieval-Augmented Generation) by allowing the assistant to semantically search ECO content and retrieve the most relevant context.\n",
    "\n",
    "**GenAI Capabilities Demonstrated**:\n",
    "✅ Embeddings\n",
    "✅ Vector store / vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Build a RAG-Powered Q&A Assistant with Few-Shot Prompting\n",
    "In this step, I set up the core Q&A system using LangChain’s RetrievalQA and Google’s Gemini (gemini-1.5-flash). The assistant uses Retrieval-Augmented Generation (RAG) with few-shot prompting to answer structured questions about ECO documents.\n",
    "\n",
    "What’s happening:\n",
    "\n",
    "-  ChromaDB retrieves relevant document chunks using vector similarity\n",
    "\n",
    "-  Gemini processes the context and returns structured answers\n",
    "\n",
    "-  Responses include: **Title, Description of Change, Reason for Change, Affected Parts, Effective Date**.\n",
    "\n",
    "*Why few-shot prompting*\n",
    "? I added sample Q&A pairs to the prompt to:\n",
    "\n",
    "-  Standardize output format\n",
    "\n",
    "-  Reduce hallucinations\n",
    "\n",
    "-  Improve field-level accuracy\n",
    "\n",
    "If a field is missing, Gemini returns \"Not mentioned\" — expected behavior in real-world RAG use cases.\n",
    "\n",
    "This step sets the stage for structured JSON extraction in Step 8.\n",
    "\n",
    "**GenAI Capabilities Demonstrated**:\n",
    "✅ Retrieval-Augmented Generation (RAG)\n",
    "✅ Few-shot prompting\n",
    "✅ Document understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Convert Assistant Output into Structured JSON\n",
    "In this step, I reformat Gemini’s natural language answer (from Step 6) into a clean, structured JSON object.\n",
    "\n",
    "While free-form answers are readable for humans, structured outputs are critical for:\n",
    "\n",
    "-  Automation and validation workflows\n",
    "\n",
    "-  CSV/database exports\n",
    "\n",
    "-  Integration with PLM systems and engineering dashboards\n",
    "\n",
    "What’s implemented:\n",
    "\n",
    "- Gemini extracts the following fields:\n",
    "    **ECO Number, Title, Description of Change, Reason for Change, Affected Parts, Effective Date**\n",
    "\n",
    "-  Missing fields are returned as null or [] for consistency\n",
    "\n",
    "-  Comma-separated items in **Affected Parts** are parsed into proper JSON arrays\n",
    "\n",
    "This structured format enables downstream use in real-world product workflows.\n",
    "\n",
    "**GenAI Capability Demonstrated**:\n",
    "✅ Structured Output / Controlled Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Batch Processing of ECO Queries with Structured Output¶\n",
    "In this step, I demonstrate how the assistant can handle multiple ECO documents in a batch using Retrieval-Augmented Generation (RAG) and Gemini.\n",
    "\n",
    "What’s happening:\n",
    "\n",
    "- A list of structured queries is passed through the assistant\n",
    "\n",
    "- Each response is reformatted into a consistent JSON object with these fields: **ECO Number, Title, Description of Change, Reason for Change, Affected Parts, Effective Date**\n",
    "\n",
    "- Incomplete or unparseable responses are skipped to maintain data quality\n",
    "\n",
    "- Final outputs are saved as .csv and .json for downstream use\n",
    "\n",
    "✅ **Prompt improvement**: Initially, I used open-ended queries (e.g., \"What changed in ECO-100001 and why?\"), but these led to:\n",
    "\n",
    "- Inaccurate matches\n",
    "\n",
    "- Answers referencing the wrong ECO\n",
    "\n",
    "- Mixed content from multiple documents\n",
    "\n",
    "To improve reliability, I switched to structured prompts like: “For ECO-100001, extract the following fields: …”\n",
    "\n",
    "This change greatly improved extraction quality and consistency.\n",
    "\n",
    "Why this matters: Batch processing of ECO documents enables:\n",
    "\n",
    "- Automated summary generation\n",
    "\n",
    "- Population of dashboards or compliance tools\n",
    "\n",
    "- Integration into PLM workflows and audit trails\n",
    "\n",
    "**GenAI Capabilities Demonstrated**: ✅ Document understanding ✅ Retrieval-Augmented Generation (RAG) ✅ Structured Output / Controlled Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Simulated Agent Routing¶\n",
    "This step introduces a simple agent-like system that automatically decides how to respond based on the user's query format.\n",
    "\n",
    "What it does:\n",
    "\n",
    "- If the query includes keywords like \"structured\" or \"JSON\", the assistant returns a structured output\n",
    "\n",
    "- Otherwise, it responds in plain language\n",
    "\n",
    "- It uses two tools:\n",
    "\n",
    "- **`qa_tool()`** for natural language Q&A\n",
    "\n",
    "- **`structured_tool()`** for structured JSON generation\n",
    "\n",
    "Why this matters: This setup mimics agent behavior — the assistant:\n",
    "\n",
    "- Inspects user intent\n",
    "\n",
    "- Selects the right tool\n",
    "\n",
    "- Delivers the answer in the format the user expects\n",
    "\n",
    "If required information is missing, the assistant does not hallucinate — instead, it returns \"Unknown\" or []. This makes the output reliable and suitable for:\n",
    "\n",
    "- Form-fillers\n",
    "\n",
    "- Compliance systems\n",
    "\n",
    "- Dashboard pipelines\n",
    "\n",
    "Example:\n",
    "\n",
    "- \"Give me a structured JSON summary of ECO-100002\" ➤ structured output\n",
    "\n",
    "- \"What is the change in ECO-100001?\" ➤ plain language response\n",
    "\n",
    "**GenAI Capabilities Demonstrated**: ✅ Agents (tool routing) ✅ Structured Output ✅ Document understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Testing Agent Router with Multiple Queries\n",
    "In this step, I test the simulated agent from Step 10 by running a mix of natural language and structured queries through it. The agent inspects each query and automatically routes it to the appropriate tool:\n",
    "\n",
    "- **Structured Output Tool** → if query mentions \"structured\" or \"JSON\"\n",
    "\n",
    "- **Plain Language QA Tool** → otherwise\n",
    "\n",
    "**Purpose**: This simulates agent-like behavior without using the full LangChain agent framework. It’s a lightweight but effective way to:\n",
    "\n",
    "- Support **multiple output styles**\n",
    "\n",
    "- Build the backend for a chatbot, smart form-filler, or support assistant\n",
    "\n",
    "- Route queries dynamically based on user intent\n",
    "\n",
    "What’s Implemented:\n",
    "\n",
    "- A routing function chooses between plain and structured responses\n",
    "\n",
    "- Natural language answers are filtered to show only the final answer\n",
    "\n",
    "- A short delay (time.sleep(10)) is used to avoid Gemini API rate limits\n",
    "\n",
    "Why it matters: This shows how GenAI assistants can flexibly adapt to:\n",
    "\n",
    "- Different user needs\n",
    "\n",
    "- Different integration contexts (UI, API, form, chat)\n",
    "\n",
    "- Prevent hallucinations when data is incomplete (returns \"Unknown\" instead of guessing)\n",
    "\n",
    "**GenAI Capabilities Demonstrated**: ✅ Agents (tool routing) ✅ Structured output ✅ Document understanding\n",
    "\n",
    "**Note on API Rate Limits**:\n",
    "If you encounter a ResourceExhausted: 429 error, it's due to Gemini API rate limits (particularly with the free tier). This notebook includes a time.sleep(10) delay between queries to reduce the chance of hitting these limits. However, rate limiting may still occur during rapid or repeated execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12. Toggle Between Structured JSON and Natural Language Output\n",
    "This step allows the assistant to return either structured JSON or plain natural language, depending on the selected `**output_format`**:\n",
    "\n",
    "- **Structured JSON** is ideal for dashboards, automation workflows, or system integration\n",
    "- **Plain text** is more suitable for human-readable summaries or chatbot-style interactions\n",
    "The prompt is dynamically constructed based on the format, and the output is cleaned to remove markdown artifacts (such as ```json fences) for reliable downstream use.\n",
    "\n",
    "This step demonstrates the **Structured Output / Controlled Generation** capability of Generative AI. It enables the assistant to extract and format key ECO fields:\n",
    "\n",
    "- ECO Number\n",
    "- Title\n",
    "- Description of Change\n",
    "- Reason for Change\n",
    "- Affected Parts\n",
    "- Effective Date\n",
    "If a field is missing in the source context, the assistant returns `**None`** or an empty list — instead of hallucinating a value. This is intentional and supports **trustworthy outputs** for use cases involving engineering change control, audits, and compliance.\n",
    "\n",
    "✅ This format flexibility shows how GenAI can serve both human users and software systems — a crucial requirement for real-world enterprise adoption.\n",
    "\n",
    "⚠️ **Note**: Due to reaching the Gemini API quota, Steps 12 and 13 could not execute at submission time.\n",
    "\n",
    "However, the code is functional and reflects working logic tested earlier in the notebook. Please refer to previous structured and natural language outputs for successful examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13. Evaluate Assistant Accuracy Using Gemini\n",
    "\n",
    "This step demonstrates the **GenAI Evaluation** capability by asking Gemini to review the assistant’s answer and compare it to a human-written gold-standard reference.\n",
    "\n",
    "What’s Implemented:\n",
    "\n",
    "\n",
    "- A real-world ECO question is posed:  \n",
    "  *“What change was made in ECO-100002 and why?”*\n",
    "- A **gold-standard answer** is manually written to represent the correct response\n",
    "- Gemini compares the assistant’s actual answer to the reference and provides:\n",
    "  - A **factual accuracy score** (from 1 to 5)\n",
    "  - A **brief justification** for the score\n",
    "\n",
    "**Why “Gold-Standard”?**\n",
    "\n",
    "The gold-standard answer serves as a human-authored benchmark — a high-quality reference used to evaluate the model’s performance. This mirrors common practices in industry (e.g., Vertex AI) and academic evaluation frameworks.\n",
    "\n",
    "Gemini’s score of **4/5** confirms that the assistant’s answer aligns well with the intended content, showing both factual understanding and reasonable inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14. Generate a Stakeholder Email\n",
    "In this step, I demonstrate how **Generative AI** can automate internal communications by generating a professional stakeholder email from an ECO document.\n",
    "\n",
    "\n",
    "What’s Implemented:\n",
    "\n",
    "- I use **RAG (Retrieval-Augmented Generation)** to fetch context from ECO-100002\n",
    "\n",
    "- A custom prompt guides Gemini 1.5 Flash to write an internal email that includes:\n",
    "\n",
    "     - A brief summary of the change\n",
    "\n",
    "     - Reason for the change\n",
    "\n",
    "     - Affected parts or documents\n",
    "\n",
    "     - Suggested approvers\n",
    "\n",
    "     - Any risks or expected delays\n",
    "\n",
    "     - Mention of unaffected areas (e.g., enclosure, firmware)\n",
    "\n",
    "This simulates a real-world use case in **engineering change management**, where teams often need to communicate product changes across departments (e.g., supply chain, QA, design). By automating the drafting process, GenAI can:\n",
    "\n",
    "- Reduce manual effort\n",
    "\n",
    "- Improve consistency and clarity\n",
    "\n",
    "- Accelerate cross-functional alignment\n",
    "\n",
    "**Note**: The email was generated entirely from **synthetic ECO data** using Gemini and a structured prompt. It is not copied from any source. With richer input (e.g., metadata or structured fields), the output could be even more tailored.\n",
    "\n",
    "**GenAI Capabilities Demonstrated**: ✅ Document understanding (via RAG)\n",
    "\n",
    "✅ Structured prompting\n",
    "\n",
    "✅ Generative business communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "The following `ECOAssistant` class implements all the features described above. The code is organized as a comprehensive class rather than separate functions for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main ECOAssistant Class Implementation\n",
    "\n",
    "Below is the implementation of the core `ECOAssistant` class that powers this project. This class provides:\n",
    "\n",
    "- Document loading and tagging\n",
    "- Vector database creation and management\n",
    "- Q&A capabilities with few-shot examples\n",
    "- Structured JSON extraction\n",
    "- Agent-based routing between output formats\n",
    "- Evaluation of response quality\n",
    "- Stakeholder email generation\n",
    "\n",
    "The class is designed to be modular and extensible, with clear error handling and rate limit management for the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECOAssistant:\n",
    "    \"\"\"\n",
    "    ECO Assistant for analyzing Engineering Change Orders.\n",
    "    \n",
    "    This class provides a RAG-powered assistant that can answer questions about\n",
    "    Engineering Change Orders, extract structured data in JSON format, and\n",
    "    generate stakeholder communications.\n",
    "    \n",
    "    Attributes:\n",
    "        api_key (str): The Google API key for Gemini access\n",
    "        config (dict): Configuration parameters for models and retrieval\n",
    "        llm: The language model instance\n",
    "        db: Vector database for document storage and retrieval\n",
    "        qa_chain: LangChain QA chain for answering questions\n",
    "        documents (list): Raw loaded ECO documents\n",
    "        split_docs (list): Chunked documents after text splitting  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the ECO Assistant.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): Google Gemini API key\n",
    "            config (dict, optional): Configuration for models and retrieval settings.\n",
    "                                    Uses default CONFIG if None.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.config = config or CONFIG\n",
    "        self.llm = None\n",
    "        self.db = None\n",
    "        self.qa_chain = None\n",
    "        self.documents = None\n",
    "        self.split_docs = None          \n",
    "        \n",
    "        # Validate config has required fields \n",
    "        required_keys = [\"chunk_size\", \"chunk_overlap\", \"retriever_k\", \n",
    "                         \"persist_dir\", \"model\", \"embedding_model\"]\n",
    "        for key in required_keys:\n",
    "            if key not in self.config:\n",
    "                print(f\"Missing key: {key}. Using default.\") \n",
    "        \n",
    "        self.setup()\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "            Initialize the LLM and embedding models.\n",
    "            \n",
    "            Sets up the Gemini model for text generation and embedding model\n",
    "            for vector representations. Handles errors if initialization fails.\n",
    "            \"\"\"\n",
    "       \n",
    "        try:\n",
    "             # Initialize the Gemini LLM. Create the main Gemini model instance \n",
    "            self.llm = ChatGoogleGenerativeAI(\n",
    "                model=self.config[\"model\"],\n",
    "                google_api_key=self.api_key\n",
    "            )\n",
    "            # Initialize the embedding model for vectors \n",
    "            self.embedding = GoogleGenerativeAIEmbeddings(\n",
    "                model=self.config[\"embedding_model\"],\n",
    "                google_api_key=self.api_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing models: {e}\")\n",
    "            raise\n",
    "   \n",
    "    def load_documents(self, folder_path):\n",
    "        \"\"\"\n",
    "        Load and tag ECO documents from a folder.\n",
    "        \n",
    "        Reads text files from the specified folder and tags each document\n",
    "        with its ECO number extracted from the filename.\n",
    "        \n",
    "        Args:\n",
    "            folder_path (str): Path to folder containing ECO text files\n",
    "            \n",
    "        Returns:\n",
    "            list: List of tagged document texts\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If directory doesn't exist\n",
    "            NotADirectoryError: If path is not a directory\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.documents = load_and_tag_documents(folder_path)\n",
    "            return self.documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_vector_store(self):\n",
    "        \"\"\"\n",
    "        Create a vector store from loaded documents.\n",
    "        \n",
    "        Splits documents into chunks, creates embeddings, and stores them\n",
    "        in ChromaDB for semantic search and retrieval.\n",
    "        \n",
    "        Returns:\n",
    "            Chroma: ChromaDB vector store instance\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If no documents have been loaded\n",
    "        \"\"\"\n",
    "      \n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "            \n",
    "        try:\n",
    "            # Create vector DB from documents \n",
    "            self.db, self.split_docs = create_vector_db(\n",
    "                self.documents, \n",
    "                self.embedding, \n",
    "                self.config[\"persist_dir\"]\n",
    "            )\n",
    "            print(f\"\\nVector store created with {len(self.split_docs)} chunks\")\n",
    "            return self.db\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector store: {e}\")\n",
    "            raise \n",
    "    \n",
    "    def build_qa_chain(self, examples, k=None):\n",
    "       \n",
    "        \"\"\"\n",
    "        Create a question-answering chain with few-shot examples.\n",
    "        \n",
    "        Args:\n",
    "            examples (str): Few-shot examples for the prompt\n",
    "            k (int, optional): Number of documents to retrieve. Uses config value if None.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If LLM or vector store aren't initialized\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.llm or not self.db:\n",
    "            raise ValueError(\"LLM and vector store must be initialized first.\")\n",
    "            \n",
    "        # Use config value if k not specified \n",
    "        k = k or self.config.get(\"retriever_k\", 8)\n",
    "\n",
    "        # Create prompt template with examples    \n",
    "        template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "            You are assisting with reviewing ECO (Engineering Change Order) documents.\n",
    "    \n",
    "            From the given context, extract the following fields:\n",
    "            - Title\n",
    "            - Description of Change\n",
    "            - Reason for Change\n",
    "            - Affected Parts (list of strings)\n",
    "            - Effective Date (in YYYY-MM-DD format)\n",
    "    \n",
    "            Use your best judgment even if fields are implied or partially mentioned. \n",
    "            Do not say \"Not mentioned\" unless you are certain the field is missing.\n",
    "    \n",
    "            {examples}\n",
    "    \n",
    "            Context:\n",
    "            {context}\n",
    "    \n",
    "            Q: {question}\n",
    "            A:\n",
    "            \"\"\".strip()\n",
    "        )\n",
    "    \n",
    "        try:  \n",
    "            # Build retrieval QA chain                                \n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                retriever=self.db.as_retriever(search_kwargs={\"k\": k}),\n",
    "                chain_type=\"stuff\",\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\n",
    "                    \"prompt\": template.partial(examples=examples)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error building QA chain: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def query(self, question):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run a query through the QA chain.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to answer about ECO documents\n",
    "            \n",
    "        Returns:\n",
    "            dict: Result with answer and source documents\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If QA chain not initialized\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"No QA chain built. Call build_qa_chain() first.\")\n",
    "            \n",
    "        try:\n",
    "            # Use retry wrapper for API rate limits  \n",
    "            return call_with_retry(lambda: self.qa_chain.invoke(question))\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying: {e}\")\n",
    "            return {\"result\": f\"An error occurred: {str(e)}\"}\n",
    "    \n",
    "    def get_structured_output(self, question, eco_number=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get structured JSON output for a question.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Question about an ECO\n",
    "            eco_number (str, optional): ECO number. Extracted from question if None.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Structured JSON with ECO fields\n",
    "        \"\"\" \n",
    "        \n",
    "        try:\n",
    "            # Get the raw answer from QA chain using our RAG query                                      \n",
    "            result = self.query(question)\n",
    "\n",
    "            # Extract ECO number from question if not provided\n",
    "            # Using regex to find ECO-XXXXXX pattern \n",
    "            if not eco_number:\n",
    "                match = re.search(r\"(ECO-\\d+)\", question)\n",
    "                eco_number = match.group(1) if match else \"Unknown\"\n",
    "                \n",
    "            # Use Gemini to reformat as JSON\n",
    "            # This structured prompt helps ensure consistent output\n",
    "            prompt = f\"\"\"\n",
    "            You are an assistant converting ECO answers into structured JSON.\n",
    "    \n",
    "            Return a JSON object with the following fields:\n",
    "            - ECO Number\n",
    "            - Title\n",
    "            - Description of Change\n",
    "            - Reason for Change\n",
    "            - Affected Parts (list of strings)\n",
    "            - Effective Date\n",
    "    \n",
    "            Always use this ECO Number: {eco_number}\n",
    "    \n",
    "            Answer:\n",
    "            {result['result']}\n",
    "            \"\"\"\n",
    "            response = call_with_retry(lambda: self.llm.invoke(prompt))\n",
    "            output = response.content.strip()\n",
    "\n",
    "            # Clean up any markdown formatting    \n",
    "            if output.startswith(\"```json\"):\n",
    "                output = output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "            return json.loads(output)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting structured output: {e}\")\n",
    "            return {\"error\": str(e)} \n",
    "    \n",
    "    def batch_process_ecos(self, eco_numbers):\n",
    "        \n",
    "        \"\"\"\n",
    "        Process multiple ECOs and return structured results.\n",
    "        \n",
    "        Args:\n",
    "            eco_numbers (list): List of ECO numbers to process\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Results as a dataframe, also saves CSV and JSON files\n",
    "        \"\"\"\n",
    "        \n",
    "        structured_results = []\n",
    "    \n",
    "        # Create queries for each ECO   \n",
    "        queries = [\n",
    "            f\"For {eco}, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\"\n",
    "            for eco in eco_numbers\n",
    "        ]\n",
    "    \n",
    "         # Process each query  \n",
    "        for query in queries:\n",
    "            print(f\"\\nRunning query: {query}\")\n",
    "    \n",
    "            try:\n",
    "                # Get raw answer\n",
    "                result = call_with_retry(lambda: self.qa_chain.invoke(query))\n",
    "                answer = result[\"result\"]\n",
    "                print(\"Raw answer:\", answer)\n",
    "    \n",
    "                # Extract ECO number from query               \n",
    "                eco_match = re.search(r\"(ECO-\\d+)\", query)    \n",
    "                eco_number = eco_match.group(1) if eco_match else \"Unknown\"\n",
    "    \n",
    "                # Create structured JSON from answer \n",
    "                structured_prompt = f\"\"\"\n",
    "                You are an assistant that converts ECO answers into structured JSON.\n",
    "    \n",
    "                Return a JSON object with the following fields:\n",
    "                - ECO Number\n",
    "                - Title\n",
    "                - Description of Change\n",
    "                - Reason for Change\n",
    "                - Affected Parts (as a list of strings)\n",
    "                - Effective Date\n",
    "    \n",
    "                Use your best judgment to include information, even if it is implied.\n",
    "                - If a part number or product is mentioned in the answer (even without labels), include it in Affected Parts.\n",
    "                - If a specific Effective Date is not listed but a Date Issued is provided, use that as the Effective Date.\n",
    "                - Only return \"Unknown\" or an empty list if there is truly no way to infer the information.\n",
    "    \n",
    "                Always use this ECO Number: {eco_number}\n",
    "    \n",
    "                Answer:\n",
    "                {answer}\n",
    "                \"\"\"\n",
    "                response = call_with_retry(lambda: self.llm.invoke(structured_prompt))\n",
    "                output = response.content.strip()\n",
    "    \n",
    "                # Clean up markdown formatting    \n",
    "                if output.startswith(\"```json\"):\n",
    "                    output = output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "                # Skip empty outputs\n",
    "                if output in (\"{}\", \"\", \"[]\"):\n",
    "                    continue\n",
    "                    \n",
    "                # Parse JSON response \n",
    "                try:\n",
    "                    parsed = json.loads(output)\n",
    "                    structured_results.append(parsed)\n",
    "                    print(\"Structured successfully.\")\n",
    "                   \n",
    "                except Exception as e:\n",
    "                    print(\"Could not parse JSON. Skipping this item.\")\n",
    "                    print(\"Error:\", e)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {e}\")\n",
    "               \n",
    "        # Save results to files\n",
    "        df = pd.DataFrame(structured_results)\n",
    "        df.to_csv(\"eco_structured_results.csv\", index=False)\n",
    "        with open(\"eco_structured_results.json\", \"w\") as f:\n",
    "            json.dump(structured_results, f, indent=2)\n",
    "\n",
    "        print(\"\\nBatch processing complete.\")\n",
    "        print(\"Structured ECO results are displayed below.\")\n",
    "        print(\"The results are also saved as CSV and JSON files.\")\n",
    "\n",
    "        # Display the dataframe\n",
    "        display(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def qa_tool(self, query):  \n",
    "\n",
    "        \"\"\"\n",
    "        Natural language answer tool.\n",
    "        \n",
    "        Returns plain text answers focused on the specific ECO in the query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question about an ECO\n",
    "            \n",
    "        Returns:\n",
    "            str: Natural language answer\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a prompt that focuses on the specific ECO  \n",
    "        focused_prompt = f\"\"\"\n",
    "        You are an assistant answering engineering questions based only on the ECO that matches the user's query.\n",
    "        \n",
    "        Only summarize the ECO number explicitly mentioned in the question. Ignore any unrelated ECOs.\n",
    "        \n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return call_with_retry(lambda: self.qa_chain.invoke(focused_prompt))[\"result\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in qa_tool: {e}\")\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "    \n",
    "    def structured_tool(self, query):\n",
    "       \n",
    "        \"\"\"\n",
    "        Structured JSON output tool.\n",
    "        \n",
    "        Returns answers as formatted JSON for integration with other systems.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question about an ECO\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON-formatted string with structured data\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Get context from QA chain \n",
    "            context = call_with_retry(lambda: self.qa_chain.invoke(query))[\"result\"]\n",
    "\n",
    "            # Format as structured JSON \n",
    "            structured_prompt = f\"\"\"\n",
    "            You are reviewing ECO content and converting it into structured JSON.\n",
    "            \n",
    "            Extract the following fields and return a JSON object with:\n",
    "            - ECO Number\n",
    "            - Title\n",
    "            - Description of Change\n",
    "            - Reason for Change\n",
    "            - Affected Parts (as a list of strings)\n",
    "            - Effective Date\n",
    "            \n",
    "            If any field is missing or not mentioned, use \"Unknown\" or [].\n",
    "            \n",
    "            Answer:\n",
    "            {context}\n",
    "            \"\"\"\n",
    "            response = call_with_retry(lambda: self.llm.invoke(structured_prompt))\n",
    "            output = response.content.strip()\n",
    "            \n",
    "            # Clean up markdown formatting if present\n",
    "            if output.startswith(\"```json\"):\n",
    "                output = output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            return output\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": str(e)})\n",
    "    \n",
    "    def route_query(self, query):\n",
    "        \n",
    "        \"\"\"\n",
    "        Route the query to the appropriate tool based on intent.\n",
    "        \n",
    "        Automatically detects if the user wants structured JSON or natural language.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question\n",
    "            \n",
    "        Returns:\n",
    "            str: Response in either JSON or natural language format\n",
    "        \"\"\"\n",
    "\n",
    "        # Look for keywords that suggest structured output preference \n",
    "        if \"structured\" in query.lower() or \"json\" in query.lower():\n",
    "            return self.structured_tool(query)\n",
    "\n",
    "        # Default to plain language for better readability  \n",
    "        return self.qa_tool(query)\n",
    "    \n",
    "    def process_multiple_queries(self, queries, delay=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        Process multiple queries through the agent router.\n",
    "        \n",
    "        Args:\n",
    "            queries (list): List of query strings\n",
    "            delay (int): Seconds to wait between queries to avoid rate limits\n",
    "            \n",
    "        Returns:\n",
    "            list: Results for each query with type and content\n",
    "        \"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for q in queries:\n",
    "            print(f\"Processing query: {q}\")\n",
    "            \n",
    "            try:\n",
    "                # Route based on query content \n",
    "                if \"structured\" in q.lower() or \"json\" in q.lower():\n",
    "                    print(\"Using structured output tool\")\n",
    "                    result = {\"type\": \"structured\", \"content\": self.structured_tool(q)}\n",
    "                else:\n",
    "                    print(\"Using natural language tool\")\n",
    "                    result_text = self.qa_tool(q)\n",
    "                    \n",
    "                    # Filter to show only the final answer line       \n",
    "                    lines = result_text.splitlines()\n",
    "                    filtered = \"\\n\".join([line for line in lines if line.startswith(\"A:\")]) or lines[-1]\n",
    "                    result = {\"type\": \"natural_language\", \"content\": filtered}\n",
    "                    \n",
    "                results.append(result)\n",
    "                \n",
    "                # Add a delay between queries to avoid rate limits   \n",
    "                if delay > 0 and queries.index(q) < len(queries) - 1:\n",
    "                    print(f\"Waiting {delay} seconds before next query...\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {e}\")\n",
    "                results.append({\"type\": \"error\", \"content\": str(e)}) \n",
    "        \n",
    "        return results \n",
    "    \n",
    "    def get_formatted_output(self, query, output_format=\"json\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get output in either JSON or plain text format.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question\n",
    "            output_format (str): Either \"json\" or \"text\"\n",
    "            \n",
    "        Returns:\n",
    "            dict or str: Response in requested format\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Getting {output_format} output for query: {query}\")\n",
    "        \n",
    "        # Run the query using RAG\n",
    "        try:\n",
    "            result = call_with_retry(lambda: self.qa_chain.invoke(query)) \n",
    "            \n",
    "            # Choose prompt style based on output format\n",
    "            if output_format == \"json\":\n",
    "                prompt = f\"\"\"\n",
    "                You are reviewing ECO data and returning a structured JSON object.\n",
    "                \n",
    "                Return these fields:\n",
    "                - ECO Number\n",
    "                - Title\n",
    "                - Description of Change\n",
    "                - Reason for Change\n",
    "                - Affected Parts\n",
    "                - Effective Date\n",
    "                \n",
    "                Context:\n",
    "                {result['result']}\n",
    "                \n",
    "                Question: {query}\n",
    "                \"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"\n",
    "                You are reviewing an ECO and responding in plain language.\n",
    "                \n",
    "                Context:\n",
    "                {result['result']}\n",
    "                \n",
    "                Question: {query}\n",
    "                \"\"\"\n",
    "            \n",
    "            # Run Gemini on the final prompt\n",
    "            final_response = call_with_retry(lambda: self.llm.invoke(prompt))\n",
    "            response_text = final_response.content.strip()\n",
    "            \n",
    "            # Clean up response formatting    \n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.strip(\"`\").strip()\n",
    "            \n",
    "            # Parse or return based on format\n",
    "            if output_format == \"json\":\n",
    "                try:\n",
    "                    return json.loads(response_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not parse JSON: {e}\")\n",
    "                    return {\"error\": str(e), \"raw_text\": response_text}\n",
    "            else:\n",
    "                return response_text\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting formatted output: {e}\")\n",
    "            return {\"error\": str(e)} if output_format == \"json\" else f\"Error: {str(e)}\"\n",
    "    \n",
    "    def evaluate_answer(self, query, reference_answer):\n",
    "        \n",
    "        \"\"\"\n",
    "        Evaluate assistant's answer against a reference.\n",
    "        \n",
    "        Uses Gemini to compare the assistant's answer to a reference answer\n",
    "        and provide a numerical score and justification.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The question to evaluate\n",
    "            reference_answer (str): Gold-standard reference answer\n",
    "            \n",
    "        Returns:\n",
    "            dict: Evaluation results with score and reason\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating answer for query: {query}\") \n",
    "        \n",
    "        try:\n",
    "            # Get the assistant's answer\n",
    "            model_answer = call_with_retry(lambda: self.qa_chain.invoke(query))[\"result\"]\n",
    "            \n",
    "            # Build evaluation prompt\n",
    "            prompt = f\"\"\"\n",
    "            You are reviewing an assistant's response.\n",
    "            \n",
    "            Compare it to the reference answer below and rate the accuracy on a scale from 1 to 5.\n",
    "            \n",
    "            Reference:\n",
    "            {reference_answer.strip()}\n",
    "            \n",
    "            Assistant:\n",
    "            {model_answer.strip()}\n",
    "            \n",
    "            Respond with:\n",
    "            Score: X\n",
    "            Reason: (short explanation)\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get evaluation from Gemini\n",
    "            response = call_with_retry(lambda: self.llm.invoke(prompt))\n",
    "            result_text = response.content.strip()\n",
    "            \n",
    "            # Parse the evaluation result\n",
    "            score_match = re.search(r\"Score:\\s*(\\d+)\", result_text)\n",
    "            reason_match = re.search(r\"Reason:\\s*(.*)\", result_text)\n",
    "            \n",
    "            score = int(score_match.group(1)) if score_match else None\n",
    "            reason = reason_match.group(1) if reason_match else \"No reason provided\"\n",
    "            \n",
    "            evaluation = {\n",
    "                \"query\": query,\n",
    "                \"reference_answer\": reference_answer,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"score\": score,\n",
    "                \"reason\": reason,\n",
    "                \"raw_result\": result_text\n",
    "            }\n",
    "            \n",
    "            print(f\"Evaluation complete. Score: {score}/5\")\n",
    "            return evaluation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating answer: {e}\") \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"reference_answer\": reference_answer,\n",
    "                \"model_answer\": \"Error retrieving model answer\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def generate_stakeholder_email(self, eco_number):\n",
    "        \n",
    "        \"\"\"  \n",
    "        Generate a stakeholder email summarizing an ECO.\n",
    "        \n",
    "        Creates a professional email that could be sent to stakeholders\n",
    "        about an engineering change, including what's changing, why,\n",
    "        and what parts are affected.\n",
    "        \n",
    "        Args:\n",
    "            eco_number (str): ECO number to summarize\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted email text\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Validate ECO number format  \n",
    "            validate_eco_number(eco_number)\n",
    "            \n",
    "            # Get ECO context from our vector DB \n",
    "            eco_query = f\"What is the change described in {eco_number}? Extract details to create a stakeholder email.\"\n",
    "            eco_context = call_with_retry(lambda: self.query(eco_query))[\"result\"]\n",
    "            \n",
    "            # Prompt for email generation with specific sections     \n",
    "            email_prompt = f\"\"\"\n",
    "            You are writing an internal stakeholder email summarizing an engineering change (ECO).\n",
    "            \n",
    "            Include the following:\n",
    "            - Intro: \"This change notification summarizes the engineering update described in {eco_number}.\"\n",
    "            - Summary of what's changing and why\n",
    "            - Affected parts or documents\n",
    "            - Who should review or approve\n",
    "            - Any risks or expected delays\n",
    "            - Mention if other areas are unaffected\n",
    "            \n",
    "            Context:\n",
    "            {eco_context}\n",
    "            \n",
    "            Email:\n",
    "            \"\"\"\n",
    "            \n",
    "            email_response = call_with_retry(lambda: self.llm.invoke(email_prompt))\n",
    "            return email_response.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError generating stakeholder email: {e}\") \n",
    "            return f\"Error generating email: {str(e)}\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Clean up resources when done with the assistant.\n",
    "        \n",
    "        Closes database connections to prevent resource leaks.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Only attempt cleanup if we have a database client\n",
    "            if self.db and hasattr(self.db, '_client'): \n",
    "                client = self.db._client\n",
    "                # Close ChromaDB connection if the method exists \n",
    "                if hasattr(client, 'close'):\n",
    "                    client.close()\n",
    "                    print(\"ChromaDB connection closed\")\n",
    "                    \n",
    "            print(\"\\nCleanup complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during cleanup: {e}\")\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cleanup()` method is important for properly closing database connections and preventing resource leaks, especially when working with vector databases that might hold open connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and Helper Functions\n",
    "\n",
    "The following sections implement:\n",
    "\n",
    "1. Configuration settings for the ECO Assistant\n",
    "2. Helper functions for loading and processing documents\n",
    "3. Vector database creation utilities\n",
    "4. API rate limit handling with exponential backoff\n",
    "5. ECO number validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for the ECO Assistant \n",
    "# These settings control chunking, retrieval, and model behavior \n",
    " \n",
    "CONFIG = {\n",
    "    \"chunk_size\": 750,                             # Size of text chunks for embedding - - small enough for context, large enough for meaning  \n",
    "    \"chunk_overlap\": 250,                          # Overlap prevents context loss between chunks   \n",
    "    \"retriever_k\": 8,                              # Number of chunks to retrieve in queries - - 8 works well for ECOs\n",
    "    \"persist_dir\": \"eco_chroma_db\",                # Vector DB storage location. Where to store our vector DB  \n",
    "    \"model\": \"models/gemini-1.5-flash\",            # LLM model. Using the faster Gemini model for better latency   \n",
    "    \"embedding_model\": \"models/embedding-001\",     # Google's text embedding model    \n",
    "}  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Loading and Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_tag_documents(folder_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load ECO documents from a folder and tag them with their ECO number.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing ECO text files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tagged document texts\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If directory doesn't exist\n",
    "        NotADirectoryError: If path is not a directory\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {folder_path}\")\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise NotADirectoryError(f\"Path is not a directory: {folder_path}\")\n",
    "    \n",
    "    # Find all text files - using glob pattern matching \n",
    "    file_paths = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    # Warn if we don't find any docs - good for debugging \n",
    "    if not file_paths:\n",
    "        print(f\"Warning: No .txt files found in {folder_path}\")\n",
    "        print(f\"Files in directory: {os.listdir(folder_path)}\")\n",
    "    \n",
    "    # Load each doc and tag it with its ECO number\n",
    "    # This helps with retrieval context later\n",
    "    documents = []\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            filename = os.path.basename(path)\n",
    "             # Pull ECO number from filename - assumes ECO-XXXXXX format  \n",
    "            eco_number = filename.split(\".\")[0] if \"ECO-\" in filename else \"Unknown-ECO\"\n",
    "            tagged_content = f\"ECO Number: {eco_number}\\n\\n{content}\"\n",
    "            documents.append(tagged_content)\n",
    "    \n",
    "    # Confirmation\n",
    "    print(f\"\\nAll {len(documents)} ECOs loaded successfully.\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed ECO Documents into a Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(documents, embedding_model, persist_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a vector database from documents.\n",
    "    \n",
    "    Splits documents into chunks and stores them in ChromaDB with embeddings.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of document texts\n",
    "        embedding_model: Embedding model to use\n",
    "        persist_dir (str): Directory to store the database\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ChromaDB instance, list of split documents)\n",
    "    \"\"\"\n",
    "      \n",
    "    # Split docs into manageable chunks for better embedding\n",
    "    # Using recursive splitter to respect natural text boundaries \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,\n",
    "        chunk_overlap=250\n",
    "    )\n",
    "    docs = [Document(page_content=d) for d in documents]\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create Chroma vector DB from our doc chunks \n",
    "    # This is where the magic happens for semantic search \n",
    "    db = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    return db, split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Rate Limit Handling\n",
    "\n",
    "When working with API-based models like Gemini, rate limiting is a common challenge. This helper function implements exponential backoff to handle rate limits gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def call_with_retry(func, max_retries=3, base_delay=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Handle API rate limits with exponential backoff.\n",
    "    \n",
    "    Retries the function call with increasing delays if rate limited.\n",
    "    \n",
    "    Args:\n",
    "        func (callable): Function to call\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        base_delay (int): Base delay in seconds (doubles each retry)\n",
    "        \n",
    "    Returns:\n",
    "        The result of the function call\n",
    "        \n",
    "    Raises:\n",
    "        Exception: Raises original exception after max retries\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            # Check specifically for rate limit errors \n",
    "            if \"ResourceExhausted\" in str(e) or \"429\" in str(e):             \n",
    "                delay = base_delay * (2 ** attempt)  \n",
    "                time.sleep(delay)\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"Max retries reached. Please try again later.\")\n",
    "                    raise\n",
    "            else:\n",
    "                # Not a rate limit - just pass through other errors \n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Validation\n",
    "\n",
    "This helper ensures that ECO numbers follow the expected format, providing clean error handling for malformed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_eco_number(eco_number):\n",
    "    \n",
    "    \"\"\"\n",
    "    Validate that the ECO number has the correct format.\n",
    "    \n",
    "    Args:\n",
    "        eco_number (str): ECO number to validate\n",
    "        \n",
    "    Returns:\n",
    "        str: The validated ECO number\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If ECO number format is invalid\n",
    "    \"\"\"\n",
    "    pattern = r\"^ECO-\\d{6}$\"\n",
    "    if not re.match(pattern, eco_number):\n",
    "        raise ValueError(f\"Invalid ECO number format: {eco_number}. Expected format: ECO-XXXXXX\")\n",
    "    return eco_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Demonstration Function\n",
    "\n",
    "The `main()` function below demonstrates the key capabilities of the ECO Assistant:\n",
    "\n",
    "1. Document loading and vector database creation\n",
    "2. Natural language Q&A\n",
    "3. Structured JSON output\n",
    "4. Batch processing of multiple ECOs\n",
    "5. Agent-based routing between output formats\n",
    "6. Output format toggling\n",
    "7. Answer evaluation against references\n",
    "8. Stakeholder email generation\n",
    "\n",
    "This provides a comprehensive demonstration of all the features implemented in the ECOAssistant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All 4 ECOs loaded successfully.\n",
      "\n",
      "Vector store created with 8 chunks\n",
      "\n",
      "Answer:\n",
      " Title: Battery Type Replacement – Lithium Polymer to Solid-State\n",
      "Description of Change: This ECO details a change in the battery type used in the MatrixSync X100 fitness tracker, replacing the existing lithium-polymer cell with a solid-state lithium battery. Documentation and BOMs will be updated accordingly.\n",
      "Reason for Change: Improve battery safety, increase product lifespan, and align with new supplier standards.\n",
      "Affected Parts: BAT-000011 | Battery – Li-Po | Rev A → Obsolete, BAT-000014 | Battery – Solid-State | New Part, BOM-000122 | MatrixSync X100 BOM | Updated battery component\n",
      "Effective Date: 2025-05-05\n",
      "\n",
      "===Simple Query===\n",
      "\n",
      "In ECO-100002, the lithium-polymer battery in the MatrixSync X100 was replaced with a solid-state battery.  The reason for this change was to improve battery safety, increase product lifespan, and align with new supplier standards.\n",
      "\n",
      "===Structured Output===\n",
      "\n",
      "Final Structured JSON:\n",
      ":\n",
      "{\n",
      "  \"ECO Number\": \"ECO-100002\",\n",
      "  \"Title\": \"Battery Type Replacement – Lithium Polymer to Solid-State\",\n",
      "  \"Description of Change\": \"This ECO details a change in the battery type used in the MatrixSync X100 fitness tracker, replacing the existing lithium-polymer cell with a solid-state lithium battery. Documentation and BOMs will be updated accordingly.\",\n",
      "  \"Reason for Change\": \"Improve battery safety, increase product lifespan, and align with new supplier standards.\",\n",
      "  \"Affected Parts\": [\n",
      "    \"BAT-000011 | Battery – Li-Po | Rev A → Obsolete\",\n",
      "    \"BAT-000014 | Battery – Solid-State | New Part\",\n",
      "    \"BOM-000122 | MatrixSync X100 BOM | Updated battery component\"\n",
      "  ],\n",
      "  \"Effective Date\": \"2025-05-05\"\n",
      "}\n",
      "\n",
      "===Batch Processing===\n",
      "\n",
      "Running query: For ECO-100001, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
      "Raw answer: Title: Enclosure Update – Add Ventilation Slots\n",
      "Description of Change: Ventilation slots were added to the top shell to improve thermal performance. This modification involves updates to the injection molding tooling for the plastic component.\n",
      "Reason for Change: Improve device thermal performance by enhancing passive airflow through the enclosure.\n",
      "Affected Parts: PRT-000210, PRT-000211\n",
      "Effective Date: 2025-05-01\n",
      "Structured successfully.\n",
      "\n",
      "Running query: For ECO-100002, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
      "Raw answer: Title: Battery Type Replacement – Lithium Polymer to Solid-State\n",
      "Description of Change: This ECO details a change in the battery type used in the MatrixSync X100 fitness tracker, replacing the existing lithium-polymer cell with a solid-state lithium battery. Documentation and BOMs will be updated accordingly.\n",
      "Reason for Change: Improve battery safety, increase product lifespan, and align with new supplier standards.\n",
      "Affected Parts: BAT-000011 | Battery – Li-Po | Rev A → Obsolete, BAT-000014 | Battery – Solid-State | New Part, BOM-000122 | MatrixSync X100 BOM | Updated battery component\n",
      "Effective Date: 2025-05-05\n",
      "Structured successfully.\n",
      "\n",
      "Running query: For ECO-100003, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
      "Raw answer: Title: Firmware Update – Step Tracking Accuracy Improvement\n",
      "Description of Change: This ECO introduces a firmware update to improve the accuracy of the step tracking algorithm for the MatrixSync X100 fitness tracker. The update refines motion detection thresholds and incorporates updated accelerometer calibration data.\n",
      "Reason for Change: User feedback and internal testing revealed inconsistencies in step count accuracy during brisk walking and slow jogging. The firmware improvement addresses these issues, improving reliability and user satisfaction.\n",
      "Affected Parts: FW-000012 | MatrixSync X100 Firmware | Rev 3.1.2 → Rev 3.2.0, LAB-000014 | Product Label – MatrixSync X100 | Rev A → Rev B\n",
      "Effective Date: 2025-05-10\n",
      "Structured successfully.\n",
      "\n",
      "Running query: For ECO-100004, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
      "Raw answer: Title: Wristband Material Change – Thermoplastic Polyurethane to Recycled Silicone\n",
      "Description of Change: This ECO proposes changing the wristband material from thermoplastic polyurethane (TPU) to a more sustainable recycled silicone compound. The change involves updates to the part specification and supplier documentation.\n",
      "Reason for Change: To improve environmental sustainability and reduce material cost without impacting product durability or comfort.\n",
      "Affected Parts: PRT-000310 | Wristband Assembly | Rev B → Rev C\n",
      "Effective Date: 2025-05-15\n",
      "Structured successfully.\n",
      "\n",
      "Batch processing complete.\n",
      "Structured ECO results are displayed below.\n",
      "The results are also saved as CSV and JSON files.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECO Number</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description of Change</th>\n",
       "      <th>Reason for Change</th>\n",
       "      <th>Affected Parts</th>\n",
       "      <th>Effective Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECO-100001</td>\n",
       "      <td>Enclosure Update – Add Ventilation Slots</td>\n",
       "      <td>Ventilation slots were added to the top shell ...</td>\n",
       "      <td>Improve device thermal performance by enhancin...</td>\n",
       "      <td>[PRT-000210, PRT-000211]</td>\n",
       "      <td>2025-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECO-100002</td>\n",
       "      <td>Battery Type Replacement – Lithium Polymer to ...</td>\n",
       "      <td>This ECO details a change in the battery type ...</td>\n",
       "      <td>Improve battery safety, increase product lifes...</td>\n",
       "      <td>[BAT-000011, BAT-000014, BOM-000122, MatrixSyn...</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECO-100003</td>\n",
       "      <td>Firmware Update – Step Tracking Accuracy Impro...</td>\n",
       "      <td>This ECO introduces a firmware update to impro...</td>\n",
       "      <td>User feedback and internal testing revealed in...</td>\n",
       "      <td>[FW-000012, MatrixSync X100 Firmware, LAB-0000...</td>\n",
       "      <td>2025-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECO-100004</td>\n",
       "      <td>Wristband Material Change – Thermoplastic Poly...</td>\n",
       "      <td>This ECO proposes changing the wristband mater...</td>\n",
       "      <td>To improve environmental sustainability and re...</td>\n",
       "      <td>[PRT-000310, Wristband Assembly]</td>\n",
       "      <td>2025-05-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ECO Number                                              Title  \\\n",
       "0  ECO-100001           Enclosure Update – Add Ventilation Slots   \n",
       "1  ECO-100002  Battery Type Replacement – Lithium Polymer to ...   \n",
       "2  ECO-100003  Firmware Update – Step Tracking Accuracy Impro...   \n",
       "3  ECO-100004  Wristband Material Change – Thermoplastic Poly...   \n",
       "\n",
       "                               Description of Change  \\\n",
       "0  Ventilation slots were added to the top shell ...   \n",
       "1  This ECO details a change in the battery type ...   \n",
       "2  This ECO introduces a firmware update to impro...   \n",
       "3  This ECO proposes changing the wristband mater...   \n",
       "\n",
       "                                   Reason for Change  \\\n",
       "0  Improve device thermal performance by enhancin...   \n",
       "1  Improve battery safety, increase product lifes...   \n",
       "2  User feedback and internal testing revealed in...   \n",
       "3  To improve environmental sustainability and re...   \n",
       "\n",
       "                                      Affected Parts Effective Date  \n",
       "0                           [PRT-000210, PRT-000211]     2025-05-01  \n",
       "1  [BAT-000011, BAT-000014, BOM-000122, MatrixSyn...     2025-05-05  \n",
       "2  [FW-000012, MatrixSync X100 Firmware, LAB-0000...     2025-05-10  \n",
       "3                   [PRT-000310, Wristband Assembly]     2025-05-15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Agent Routing===\n",
      "\n",
      "Structured JSON Output:\n",
      "{\n",
      "  \"ECO Number\": \"ECO-100002\",\n",
      "  \"Title\": \"Battery Type Replacement – Lithium Polymer to Solid-State\",\n",
      "  \"Description of Change\": \"Replaced lithium-polymer battery with solid-state battery in the MatrixSync X100. BOMs and documentation updated.\",\n",
      "  \"Reason for Change\": \"Improve battery safety, increase product lifespan, and align with new supplier standards.\",\n",
      "  \"Affected Parts\": [\n",
      "    \"BAT-000011 | Battery – Li-Po | Rev A → Obsolete\",\n",
      "    \"BAT-000014 | Battery – Solid-State | New Part\",\n",
      "    \"BOM-000122 | MatrixSync X100 BOM | Updated battery component\"\n",
      "  ],\n",
      "  \"Effective Date\": \"2025-05-05\"\n",
      "}\n",
      "\n",
      "Natural Language Answer:\n",
      "ECO-100001 adds ventilation slots to the top shell of the enclosure to improve thermal performance.  This involves updating the injection molding tooling for the plastic component.  The affected part is PRT-000210, and the effective date is 2025-05-01.\n",
      "\n",
      "===Multiple Queries===\n",
      "\n",
      "Query: What change was made in ECO-100001?\n",
      "\n",
      "Natural Language Answer:\n",
      "In ECO-100001, ventilation slots were added to the top shell of the enclosure to improve thermal performance.  This involved updating the injection molding tooling for the plastic component.  The affected part is PRT-000210.\n",
      "\n",
      "Query: Give me a structured summary of ECO-100002.\n",
      "\n",
      "Structured Output:\n",
      "{\n",
      "  \"ECO Number\": \"ECO-100002\",\n",
      "  \"Title\": \"Battery Type Replacement – Lithium Polymer to Solid-State\",\n",
      "  \"Description of Change\": \"Replaced lithium-polymer battery with solid-state battery in the MatrixSync X100. BOMs and documentation updated.\",\n",
      "  \"Reason for Change\": \"Improve battery safety, increase product lifespan, and align with new supplier standards.\",\n",
      "  \"Affected Parts\": [\n",
      "    \"BAT-000011 | Battery – Li-Po | Rev A → Obsolete\",\n",
      "    \"BAT-000014 | Battery – Solid-State | New Part\",\n",
      "    \"BOM-000122 | MatrixSync X100 BOM | Updated battery component\"\n",
      "  ],\n",
      "  \"Effective Date\": \"2025-05-05\"\n",
      "}\n",
      "\n",
      "Query: For ECO-100004, extract the field Affected Parts only.\n",
      "\n",
      "Natural Language Answer:\n",
      "PRT-000310 | Wristband Assembly | Rev B → Rev C\n",
      "\n",
      "=== Format Toggle ===\n",
      "\n",
      "Getting json output for query: What change was made in ECO-100002 and why?\n",
      "Response:\n",
      "{'Affected Parts': ['Lithium-polymer battery'],\n",
      " 'Description of Change': 'Replaced lithium-polymer battery with a solid-state '\n",
      "                          'battery in the MatrixSync X100.',\n",
      " 'ECO Number': 'ECO-100002',\n",
      " 'Effective Date': None,\n",
      " 'Reason for Change': 'Improve battery safety, increase product lifespan, and '\n",
      "                      'align with new supplier standards.',\n",
      " 'Title': 'MatrixSync X100 Battery Replacement'}\n",
      "\n",
      "=== Answer Evaluation ===\n",
      "\n",
      "Evaluating answer for query: What change was made in ECO-100002 and why?\n",
      "Evaluation complete. Score: 4/5\n",
      "\n",
      "Evaluation Result:\n",
      " Score: 4\n",
      "Reason:The assistant correctly identifies the battery change and the device.  It adds some plausible reasons for the change, although these are not explicitly stated in the reference.  The ECO number is an addition not present in the reference, but doesn't detract from the overall accuracy.\n",
      "\n",
      "=== Stakeholder Email ===\n",
      "\n",
      "Subject: ECO-100002: Battery Replacement in MatrixSync X100\n",
      "\n",
      "Dear Stakeholders,\n",
      "\n",
      "This change notification summarizes the engineering update described in ECO-100002.\n",
      "\n",
      "Effective May 5th, 2025, we are replacing the lithium-polymer battery (BAT-000011) in the MatrixSync X100 fitness tracker with a solid-state lithium battery (BAT-000014).  The obsolete BAT-000011 is being replaced due to three key factors: improved battery safety, increased product lifespan, and alignment with new supplier standards.  This upgrade significantly enhances the overall safety and longevity of the MatrixSync X100.\n",
      "\n",
      "**Affected Parts and Documents:**\n",
      "\n",
      "* **Part:** BAT-000011 (obsolete) replaced with BAT-000014\n",
      "* **Document:** Bill of Materials (BOM-000122) – Requires update to reflect the new battery part number.\n",
      "\n",
      "**Review and Approval:**\n",
      "\n",
      "Please review the full ECO document (ECO-100002) and provide your approval by [Date - allow reasonable time].  [Name/Title] is the designated approver for [Area of Responsibility], and [Name/Title] for [Area of Responsibility].\n",
      "\n",
      "**Risks and Potential Delays:**\n",
      "\n",
      "We anticipate a smooth transition; however, minor delays in production are possible during the initial phase of the changeover. We are working closely with our manufacturing partners to minimize any disruption.  A contingency plan is in place to mitigate potential delays.\n",
      "\n",
      "**Unaffected Areas:**\n",
      "\n",
      "This ECO only affects the battery component and the associated BOM.  All other aspects of the MatrixSync X100 design and functionality remain unchanged.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name/Department]\n",
      "\n",
      "Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to demonstrate ECO Assistant capabilities.\n",
    "    \n",
    "    Loads documents, creates a vector store, and demonstrates various\n",
    "    features including Q&A, structured output, batch processing,\n",
    "    and stakeholder email generation.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Create an instance of the ECOAssistant class with API key and config \n",
    "    eco_assistant = ECOAssistant(api_key=GOOGLE_API_KEY, config=CONFIG)\n",
    "   \n",
    "\n",
    "    # Load the synthetic ECO docs from the project structure\n",
    "    # Try to find docs in standard project locations\n",
    "    docs_paths = [\"./SYNT_DOCS\", \"../SYNT_DOCS\", \"SYNT_DOCS\"]\n",
    "    docs_loaded = False\n",
    "\n",
    "    for path in docs_paths:\n",
    "        if os.path.exists(path):\n",
    "            documents = eco_assistant.load_documents(path)\n",
    "            docs_loaded = True\n",
    "            break\n",
    "\n",
    "    if not docs_loaded:\n",
    "        raise FileNotFoundError(\"Could not find SYNT_DOCS folder. Please check the path.\")\n",
    "    \n",
    "\n",
    "    # Create vector store for semantic search \n",
    "    eco_assistant.create_vector_store()\n",
    "\n",
    "    # Add few-shot examples to improve extraction quality \n",
    "    # Few-shot examples really help with extraction consistency\n",
    "    # These ECO templates guide the model's output format                        \n",
    "    examples = \"\"\"\n",
    "    Q: For ECO-100001, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
    "    A:\n",
    "    Title: Enclosure Update – Add Ventilation Slots\n",
    "    Description of Change: Ventilation slots were added to the top shell to improve thermal performance.\n",
    "    Reason for Change: Improve thermal performance.\n",
    "    Affected Parts: PRT-000210\n",
    "    Effective Date: 2025-05-01\n",
    "\n",
    "    Q: For ECO-100002, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\n",
    "    A:\n",
    "    Title: Battery Type Replacement – Lithium Polymer to Solid-State\n",
    "    Description of Change: Replaced lithium-polymer battery with solid-state battery in the MatrixSync X100. BOMs and documentation updated.\n",
    "    Reason for Change: Improve battery safety, increase product lifespan, and align with new supplier standards.\n",
    "    Affected Parts: BAT-000011 | Battery – Li-Po | Rev A → Obsolete, BAT-000014 | Battery – Solid-State | New Part, BOM-000122 | MatrixSync X100 BOM | Updated battery component\n",
    "    Effective Date: 2025-05-05\n",
    "    \"\"\"\n",
    "    eco_assistant.build_qa_chain(examples)\n",
    "    \n",
    "    # Test standard Q&A functionality\n",
    "    # DEMO 1: Basic extraction test \n",
    "    # Let's try getting structured fields from ECO-100002\n",
    "    test_question = \"For ECO-100002, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\"\n",
    "    test_result = eco_assistant.query(test_question)\n",
    "    print(\"\\nAnswer:\\n\", test_result[\"result\"])\n",
    "    \n",
    "    # Test different functionalities\n",
    "    # DEMO 2: Natural language query \n",
    "    # More conversational style question       \n",
    "    print(\"\\n===Simple Query===\\n\")\n",
    "    result = eco_assistant.query(\"What change was made in ECO-100002 and why?\")\n",
    "    print(result[\"result\"])  \n",
    "\n",
    "    # DEMO 3: JSON formatting demo  \n",
    "    # Same info but in structured format for systems integration        \n",
    "    print(\"\\n===Structured Output===\")\n",
    "    structured_result = eco_assistant.get_structured_output(\n",
    "        \"For ECO-100002, extract the following fields: Title, Description of Change, Reason for Change, Affected Parts, and Effective Date.\"\n",
    "    )\n",
    "    print(\"\\nFinal Structured JSON:\\n:\") \n",
    "    print(json.dumps(structured_result, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # DEMO 4: Batch processing for PLM integration\n",
    "    # Process multiple ECOs in one shot - great for automation\n",
    "    print(\"\\n===Batch Processing===\")\n",
    "    eco_numbers = [\"ECO-100001\", \"ECO-100002\", \"ECO-100003\", \"ECO-100004\"]\n",
    "    batch_results = eco_assistant.batch_process_ecos(eco_numbers)\n",
    "    \n",
    "    # DEMO 5: Agent-style routing based on query intent\n",
    "    # System auto-detects if user wants JSON or plain text\n",
    "    print(\"\\n===Agent Routing===\") \n",
    "    structured_query = \"Give me a structured JSON summary of ECO-100002\"\n",
    "    plain_query = \"What is the change in ECO-100001?\"\n",
    "    print(\"\\nStructured JSON Output:\")\n",
    "    print(eco_assistant.route_query(structured_query))\n",
    "    print(\"\\nNatural Language Answer:\") \n",
    "    print(eco_assistant.route_query(plain_query))  \n",
    "    \n",
    "    # DEMO 6: Multiple query handling\n",
    "    # Shows how this works with a sequence of different questions\n",
    "    print(\"\\n===Multiple Queries===\")\n",
    "    queries = [\n",
    "        \"What change was made in ECO-100001?\",\n",
    "        \"Give me a structured summary of ECO-100002.\",\n",
    "        \"For ECO-100004, extract the field Affected Parts only.\"\n",
    "    ]\n",
    "    \n",
    "    for q in queries:\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        try:\n",
    "            # Look for signal words that indicate JSON preference \n",
    "            if \"structured\" in q.lower() or \"json\" in q.lower():\n",
    "                print(\"\\nStructured Output:\")\n",
    "                result = eco_assistant.route_query(q)\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"\\nNatural Language Answer:\")\n",
    "                result_text = eco_assistant.route_query(q)\n",
    "                # Clean up response to just show the answer part \n",
    "                lines = result_text.splitlines()\n",
    "                filtered = \"\\n\".join([line for line in lines if line.startswith(\"A:\")]) or lines[-1]\n",
    "                print(filtered)\n",
    "\n",
    "            # Add a small delay between queries\n",
    "            # Gemini gets grumpy with rapid-fire requests                       \n",
    "            if queries.index(q) < len(queries) - 1:\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping due to error: {e}\") \n",
    "\n",
    "    # DEMO 7: Format toggling for integration flexibility    \n",
    "    print(\"\\n=== Format Toggle ===\\n\")\n",
    "    user_query = \"What change was made in ECO-100002 and why?\"\n",
    "    output_format = \"json\"\n",
    "    json_result = eco_assistant.get_formatted_output(user_query, output_format)\n",
    "    print(\"Response:\")\n",
    "    if output_format == \"json\":\n",
    "        pprint(json_result)\n",
    "    else:\n",
    "        print(json_result)\n",
    "    \n",
    "    # DEMO 8: Assistant evaluation vs. reference answers\n",
    "    # This is how we'd check accuracy in a real implementation\n",
    "    print(\"\\n=== Answer Evaluation ===\\n\")\n",
    "    evaluation_query = \"What change was made in ECO-100002 and why?\"\n",
    "    # Gold standard answer for comparison \n",
    "    gold_answer = \"\"\"\n",
    "    The lithium-polymer battery in the MatrixSync X100 was replaced with a solid-state lithium battery \n",
    "    to improve safety and performance. The reason for this change was not explicitly stated in the ECO.\n",
    "    \"\"\"\n",
    "    evaluation = eco_assistant.evaluate_answer(evaluation_query, gold_answer)\n",
    "    print(\"\\nEvaluation Result:\\n\", evaluation[\"raw_result\"])\n",
    "\n",
    "    # DEMO 9: Stakeholder email - business communication\n",
    "    # Generate nice email to send to teams about the change   \n",
    "    print(\"\\n=== Stakeholder Email ===\\n\")\n",
    "    email = eco_assistant.generate_stakeholder_email(\"ECO-100002\")\n",
    "    print(email)\n",
    "    \n",
    "    # Clean up resources to prevent leaks. Close the DB connection               \n",
    "    eco_assistant.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Attribution & Original Contributions\n",
    "\n",
    "This project builds upon foundation components while adding significant custom functionality:\n",
    "\n",
    "#### Foundation (from Google GenAI Code Labs):\n",
    "- Gemini integration via langchain-google-genai\n",
    "- ChromaDB setup for vector-based document retrieval\n",
    "- Prompt engineering patterns for structured outputs\n",
    "\n",
    "#### My Original Contributions:\n",
    "- Creation and tagging of synthetic ECO documents\n",
    "- Custom prompts for field extraction and email generation\n",
    "- Agent routing logic between plain Q&A and JSON output\n",
    "- Batch processing with schema enforcement\n",
    "- Model-based response evaluation system\n",
    "- Comprehensive error handling and API rate limit management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "\n",
    "This project demonstrates how Gemini-powered Generative AI can transform engineering workflows by automating the understanding and summarization of unstructured technical documents.\n",
    "\n",
    "The ECO Assistant combines:\n",
    "- Document understanding through vector embeddings\n",
    "- Few-shot prompting for consistent extraction\n",
    "- Structured outputs for system integration\n",
    "- Intelligent format selection based on user needs\n",
    "- Professional communication generation\n",
    "\n",
    "Despite using synthetic data, this assistant reflects real-world engineering requirements and showcases practical GenAI application in a domain-specific context.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook was created for the Google GenAI Capstone Challenge (Q1 2025)."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7186204,
     "sourceId": 11467397,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
